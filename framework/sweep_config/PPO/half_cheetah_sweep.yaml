program: prefppo.py
entity: cleanRLHF
project: HalfCheetah_PPO_common_tuning
method: bayes
metric:
  goal: maximize
  name: "evaluate/mean"
parameters:
  seed:
    values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  torch_deterministic:
    values: [true]
  cuda:
    values: [true]
  track:
    values: [true]
  wandb_project_name:
    values: [""]
  wandb_entity:
    values: [""]
  capture_video:
    values: [false]
  render_mode:
    values: [""]
  num_envs:
    values: [1]
  log_file:
    values: [true]
  log_level:
    values: ["INFO"]

  env_id:
    values: ["HalfCheetah-v5"]
  total_timesteps:
    values: [1000000]
  learning_rate:
    min: 0.0001
    max: 0.001
    distribution: uniform
  num_steps:
    values: [2048]
  gamma:
    min: 0.99
    max: 0.995
    distribution: uniform
  anneal_lr:
    values: [true]
  gae_lambda:
    values: [0.95]
  num_minibatches:
    values: [32]
  update_epochs:
    values: [10]
  norm_adv:
    values: [true]
  clip_coef:
    values: [0.2]
  clip_vloss:
    values: [true]
  vf_coef:
    values: [0.5]
  max_grad_norm:
    values: [0.5]

  evaluation_frequency:
    values: [10000]
  evaluation_episodes:
    values: [10]

  early_stop:
    values: [false]
  early_stopping_step:
    values: [500000]
  early_stop_patience:
    values: [5]
  early_stopping_mean:
    values: [0]
  enable_greater_or_smaller_check:
    values: [false]

  reward_net_hidden_dim:
    values: [64, 128, 256, 512]
  reward_net_hidden_layers:
    values: [2, 3, 4, 5]
  reward_net_val_split:
    values: [0.2]
  reward_net_dropout:
    values: [0, 0.1, 0.2, 0.3]
  agent_net_hidden_dim:
    values: [128, 256, 512]
  agent_net_hidden_layers:
    values: [3, 4, 5]

  feedback_server_url:
    values: [ "http://localhost:5001" ]
  feedback_server_autostart:
    values: [ false ]
  teacher_feedback_mode:
    values: [ "simulated" ]

  teacher_feedback_num_queries_per_session:
    values: [20, 30, 40, 50, 60]
  teacher_feedback_total_queries:
    values: [500, 1000, 1500, 2000, 3000]
  teacher_feedback_schedule:
    values:
    - linear
    - exponential
  teacher_feedback_exponential_lambda:
    values: [0.1]
  teacher_feedback_batch_size:
    values: [16, 32]
  teacher_update_epochs:
    distribution: int_uniform
    max: 20
    min: 10
  teacher_batch_strategy:
    values:
    - minibatch
    - batch
    - full
  teacher_minibatch_size:
    values: [5, 10, 20]
  teacher_learning_rate:
    min: 0.0005
    max: 0.001
    distribution: uniform
  trajectory_length:
    values: [32, 64, 128]
  preference_sampling:
    values:
      - disagree
      - uniform
      - entropy
  teacher_sim_gamma:
    values: [1]
  teacher_sim_beta:
    values: [-1]
  teacher_sim_epsilon:
    values: [0]
  teacher_sim_delta_skip:
    values: [-1e7]
  teacher_sim_delta_equal:
    values: [0]
  pref_buffer_size_sessions:
    min: 1
    max: 20

  unsupervised_exploration:
    values: [true]
  total_explore_steps:
    values: [10000]

  surf:
    values: [true, false]
  unlabeled_batch_ratio:
    values: [1, 2, 4]
  surf_sampling_strategy:
    values:
    - uniform
    - disagree
    - entropy
  surf_tau:
    values: [0.999]
  lambda_ssl:
    values: [0.1]
  max_augmentation_offset:
    values: [10]
  min_augmentation_offset:
    values: [5]

  rune:
    values: [true, false]
  rune_beta:
    min: 0.05
    max: 0.1
    distribution: uniform
  rune_beta_decay:
    values: [0.001, 0.0001, 0.00001]
  exploration_load:
    values: [false]
  path_to_replay_buffer:
    values: [""]
  path_to_model:
    values: [""]

